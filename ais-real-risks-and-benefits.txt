An AI roundtable discussion is a staple of the tech journalismcircus usuallyframed with a preamble about dystopic threats to human existence from the inexorable rise of super intelligence machines. Just add a movie still from The Terminator.What typicallyresults from such a set-up is a tangled back and forth of viewpoints and anecdotes, where a coherent definition of AI fails to beanemergent property of theassembled learnedminds. Nor is thereclearconsensus about what AI might meanfor the future of humanity. After all, how can even the most well-intentioned groupthink predict the outcome of an unknown unknown?None of this issurprising,given we humans dont even know what human intelligence is. Thinking ourselves inside themetallic shellof machine consciousness  whatever that might mean  is about as fruitful astrying to imagine what our thoughts might be ifour own intelligence were embodied inside the flesh of a pear, rather the fleshy formswe do inhabit. Or if our consciousnessexistedfleetingly inliquid paint duringthe momentofanimation by an artists intention. Philosophers canphilosophize aboutthe implications ofAI, sure (and of course they do). But only an idiot wouldclaim to know.The panel discussion I attended this week at Londons hyper-trendy startup co-working hubSecond Home trod plenty of thisfamiliar ground. So I wont rehash the usual arguments. Rather, and as some mightargue making more like a machine  in the sense ofacting like analgorithm trained to surface novelty from a mixed data dump  Ive compiledalist (below) of some of the more interesting points that didemerge as panelists were asked to consider whether AI is a force for good (or not).Ive also listed some promising avenues for(narrow) AI mentioned by participants. Sowhere they see potential for learning algorithms to solve problemshumans might otherwise find tricky to crack  and also where those use-cases can bebroadly considered sociallybeneficial, in an effort to steer the AI narrative away from bloodthirsty robots.The last list is a summary of more groundedperceived threats/risks, i.e. those thatdont focus on the stereotypicaldoomsday scenarioof future superintelligent machines judginghumans a waste of planetary space, but which are again focused on risks associated with thekind of narrow but proliferating  in terms of applications and usage AI we do already have.One more point before switching tobullets and soundbites: the most concise description of (narrow) AI that emerged during the hour long discussion came from Tractable founderAlexandre Dalyac, who summed it up thus:Algorithms compared to humans can usually tend to solve scale, speed or accuracy issues.So there you have it: AI, its all about scale, speed andaccuracy. Not turninghumans into liquid soap. But if you do want to concern yourself withwhere machine intelligence is headed, then thinking about how algorithmic scale, speed and accuracy  applied over more and more aspects of humanlives willimpact and shape the societies we live iniscertainly a question worthpondering.PanelistsDiscussionpoints of above average interest:If the future of humanity is at stake should they be forced to open source it? Or how can we control whats happening there? askedMignot. I dont think anyone knows what Google is doing. Thats one of the issues, thats one of the worries we should have.A movement to open source machine learning-related research could also be a way to lessen public fears about the futureimpact of AItechnologies, added Jun.One of the interesting philosophical questions is whether your ability to do a particular task with absolute focus  and reduce the false positives, increase the safety  actually requires a narrow form of intelligence. And at the point where our machines start to become more general, and sort of inherently more human-like, whether necessarily that introduces a reduction in safety, positedMedlock.I can imagine that the kind of flexibility of the human brain, the plasticity to respond to so many different scenarios requires a reduction in specific abilities to do particular tasks. I think thats going to be one of the interesting things that will emerge as we start to develop AGI [artificial general intelligence]  whether actually it becomes useful for a very different set of reasons to narrow AI.I dont think artificial intelligence in itself is what I would be concerned about, its more artificial stupidity. Its the stupidity that comes with either a narrow focus, or a misunderstanding of the broader issues, addedErden. The difficulty in trying to establish all the little details that make up the context in which individual specific tasks happen.Once you try to ask individual programs to do very big things, and they need therefore to take into account lots of issues, then it becomes much more difficult.The guys who built the Web put it up and out there and didnt really think about the ethics at all. Didnt think about putting those tools into the hands of people who would use those tools negatively, instead of positively. And I think we can take those lessons and apply them to new technologies, arguedKing.A good example for the Web would be people believing that the laws of California were appropriate to everywhere around the world. And they arent, and they werent, and actually it took those Web companies a huge amount of time  and it was peer group pressure, lobby groups and so on  in order to get those organizations to behave actually appropriately for the laws of those individual countries they were operating in.Im a bit puzzled that people talk about AI ethics, addedChace. Machines may well be moral beings at some point but at the moment its not about ethics, its about safety. Its about making sure that as AIs get more and more powerful that they are safe for humans. They dont care about us, they dont care about anything. They dont know they exist. But they can do us damage, or they can provide benefits and we need to thinking about how to make them safe.Id suggest whenever AI comes in, even potentially to replace labour, its genuinely because its an efficiency gain  so creating more. But then perhaps the way to think about it is how this efficiency gain is distributed. So if its concentrated in the hands of the owners perhaps that tends to be not of good value to society. But if the benefits accrue to society at large thats potentially better, saidDalyac.For example something that were working on is automating a task in the visual assessment of insurance claims. And the benefit of that would be to lower insurance premiums for car insurance so this would be a case where the people who are usually employed to do this would find themselves out of work, so that might involve maybe400 people in this country. But as a result you have 50 million people thatbenefit.Should it be a discipline at school where students would learn about AI? askedMignot. Could it be interesting to have classes around one step further. Once you know how to code a computer in a binary language, what does it mean to create an intelligent device?I think that would help a lot with the discussion because today coders dont really understand the limitations and the potential of technology. What does it mean to be a machine that can learn by itself and make decisions? Its so abstract as a concept that I think for people who are not working in the field its either too opaque to even consider, or really scary.Were asking people to understand something that weve not really understood ourselves, or classified at least. So, when were talking about smartphones were not really talking about AI, were talking about some clever computing. Were talking about some very interesting programming and the possibility that this programming can learn and adapt but in very, very simple ways, said Erden.When you describe it like that to people I dont think theyre either scared by it or fail to understand it. But if you describe this under the umbrella term of AI you promise too much, you disappoint a lot and you also confuse people Whats wrong with saying clever computing? Whats wrong with saying clever programming? Whats wrong with saying computational intelligence?I would say that if you take a look at the papers youll realize that Watson might just be pure branding. All it is is a very large team of researchers that have done really well on a single task, and have said hey lets call it Watson, and lets make it this super intelligent being, so the next time they ask us to do something intelligent well get the same researchers, or similar researchers to work on something else, arguedDalyac.Were looking at automating the assessment of damage on cars, and theres a paper by IBM Watson in 2012 which, to be honest, uses very, very old school AI  and AI that I can say for sure has nothing to do with winning at Jeopardy, he added.Promising applications for learning algorithms cited during the roundtable:Some near-term concerns about the proliferation of machine learning plus big data: Over the lastdecade or so the use of data has largely been something that happens below the surface. And users data gets passed around and fed to targeting networks and I think, and to some degree I hope, there will be a change over the next ten years or so where partly people become aware that the data that is collected, that characterizes the things they do, their likes and interests, that thats an asset that actually is theirs to own and control, arguedMedlock.Moving towards consumers thinking about data a little bit like a currency in the same way that they use and own their own money, and that theyre able to make decisions about where they share that data Moving the processing, manipulation and storage of data from the murky depths, to something that people are at least aware of and can make decisions about intentionally.That will continue to be a challenge, for governments, for industry, for academia. Were not going to solve that one quickly but there are a lot of people thinking hard about that, said Crow. If you look at some of the regulatory stuff thats happening, certainly in the EU and starting to happen in the US as well, I think you are seeing people at least understanding theres a concern there now.And that this is an area where government needs to play an effective role. I dont think we know exactly what that looks like yet  I dont think weve finished that discussion. But at least a discussion is happening now and I think thats really important.A survey of U.K. users conducted by SwiftKey ahead of the panel discussionfound that fear of jobs beingmade redundant by advances in AI was of concern tothe majority (52 per cent) of respondents. Whilejust a third (36 per cent) said they want to see AI having a bigger role in society  implying thattwo-thirds would prefer checks and balances on theproliferation of machine learning technologies.Bottom line, if increasing algorithmic efficiency is destroying more jobs than its creating then massive social re-structuring is inevitable. So human brains seekingto ask questions about who benefits from such accelerated change, and what kind of society people want to live in, is surely justprudent due diligence  not to mentionthe very definition of(biological) intelligence.