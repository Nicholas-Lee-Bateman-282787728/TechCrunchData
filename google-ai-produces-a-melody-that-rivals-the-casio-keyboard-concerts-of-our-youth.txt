Its 1989. Your parents are watching the Berlin Wall being torn down, but youre upstairs in your room, fresh batteries in your Casio SA-1, prepping for your concert. The built-in beats are hot. Youve been listening to the Wake me up before you go-go demo song and you think youve been inspired. You select instrument number 02, HONKY-TONK PIANO. Its time to tickle the plastic ivories and burn this mother down. Hit it!The beat drops here ^Pretty good, right? But what if I told you that this was not an obsessively practiced performance by a precocious 7-year-old, but the original creation of an artistic artificial intelligence, the work of the boffins at Google Research (specifically, and naturally, Google Brain)? You probably wouldnt be surprised, because thats in the headline. But imagine the impact if youd skipped the headline and went straight to the body of the article ().We believe that the models that have worked so well in speech recognition, translation and image annotation will seed an exciting new crop of tools for art and music creation, wrote Eck.Like any other creative entity, Magenta needs to get its work out there, and it needs feedback. To that end, Google is soliciting creatives and coders both to join the community, check out the code, feed it data and so on. The project will live on both GitHub and use Googles own open-source machine learning platform TensorFlow. So get in there and fork it.Its far from the first time researchers have looked at the possibility of computer-generated music (research stretches back decades), but with the resources and brainpower of Google Research behind it, Magenta could be one of the more interesting efforts to do so.You can follow along with the teams progress at the (currently minimal) Magenta blog.